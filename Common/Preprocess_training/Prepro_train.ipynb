{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51febb6a-0bcc-4e28-903d-69513de23c66",
   "metadata": {},
   "source": [
    "Preprocess the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88ab95e-f44d-4f0b-a851-a666ca4a2d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1: 100%|█████████████████████████████████████████████████████████████| 1169/1169 [01:14<00:00, 15.73image/s]\n",
      "Processing 10: 100%|████████████████████████████████████████████████████████████| 1052/1052 [01:04<00:00, 16.19image/s]\n",
      "Processing 11: 100%|████████████████████████████████████████████████████████████| 1038/1038 [01:06<00:00, 15.70image/s]\n",
      "Processing 12: 100%|████████████████████████████████████████████████████████████| 1019/1019 [01:04<00:00, 15.81image/s]\n",
      "Processing 13: 100%|████████████████████████████████████████████████████████████| 1158/1158 [01:13<00:00, 15.68image/s]\n",
      "Processing 14: 100%|████████████████████████████████████████████████████████████| 1060/1060 [01:05<00:00, 16.08image/s]\n",
      "Processing 15: 100%|████████████████████████████████████████████████████████████| 1110/1110 [01:11<00:00, 15.59image/s]\n",
      "Processing 16: 100%|████████████████████████████████████████████████████████████| 1033/1033 [01:05<00:00, 15.88image/s]\n",
      "Processing 17: 100%|████████████████████████████████████████████████████████████| 1069/1069 [01:04<00:00, 16.47image/s]\n",
      "Processing 18: 100%|████████████████████████████████████████████████████████████| 1173/1173 [01:13<00:00, 15.96image/s]\n",
      "Processing 19: 100%|████████████████████████████████████████████████████████████| 1140/1140 [01:10<00:00, 16.08image/s]\n",
      "Processing 2: 100%|█████████████████████████████████████████████████████████████| 1060/1060 [01:05<00:00, 16.18image/s]\n",
      "Processing 20: 100%|████████████████████████████████████████████████████████████| 1181/1181 [01:13<00:00, 16.02image/s]\n",
      "Processing 21: 100%|████████████████████████████████████████████████████████████| 1066/1066 [01:06<00:00, 16.11image/s]\n",
      "Processing 22: 100%|████████████████████████████████████████████████████████████| 1123/1123 [01:11<00:00, 15.63image/s]\n",
      "Processing 23: 100%|████████████████████████████████████████████████████████████| 1035/1035 [01:03<00:00, 16.19image/s]\n",
      "Processing 24: 100%|████████████████████████████████████████████████████████████| 1050/1050 [01:05<00:00, 16.02image/s]\n",
      "Processing 25: 100%|████████████████████████████████████████████████████████████| 1100/1100 [01:06<00:00, 16.62image/s]\n",
      "Processing 26: 100%|████████████████████████████████████████████████████████████| 1119/1119 [01:07<00:00, 16.60image/s]\n",
      "Processing 27: 100%|████████████████████████████████████████████████████████████| 1130/1130 [01:08<00:00, 16.51image/s]\n",
      "Processing 3: 100%|█████████████████████████████████████████████████████████████| 1099/1099 [01:07<00:00, 16.25image/s]\n",
      "Processing 4: 100%|█████████████████████████████████████████████████████████████| 1038/1038 [01:04<00:00, 16.11image/s]\n",
      "Processing 5: 100%|█████████████████████████████████████████████████████████████| 1020/1020 [01:06<00:00, 15.39image/s]\n",
      "Processing 6: 100%|█████████████████████████████████████████████████████████████| 1246/1246 [01:16<00:00, 16.35image/s]\n",
      "Processing 7: 100%|█████████████████████████████████████████████████████████████| 1055/1055 [01:04<00:00, 16.24image/s]\n",
      "Processing 8: 100%|█████████████████████████████████████████████████████████████| 1027/1027 [00:59<00:00, 17.30image/s]\n",
      "Processing 9: 100%|█████████████████████████████████████████████████████████████| 1105/1105 [01:08<00:00, 16.19image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean up Done\n",
      "Store the extracted hand_landmarks.npy and hand_labels.npy Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    ")\n",
    "\n",
    "# Set input and output directories\n",
    "input_dir = \"Letters_Dataset\"\n",
    "output_dir = \"Letters_Dataset_Output\"\n",
    "\n",
    "# Initialize empty lists to store landmarks and labels\n",
    "landmarks = []\n",
    "labels = []\n",
    "\n",
    "# Loop through subdirectories in input directory\n",
    "for subdir in os.listdir(input_dir):\n",
    "    subdir_path = os.path.join(input_dir, subdir)\n",
    "\n",
    "    # Check if subdirectory is valid and has images\n",
    "    if os.path.isdir(subdir_path):\n",
    "        image_files = [os.path.join(subdir_path, f) for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))]\n",
    "        if not image_files:\n",
    "            continue\n",
    "        \n",
    "        # Loop through images in subdirectory\n",
    "        for image_path in tqdm(image_files, desc=f\"Processing {subdir}\", unit=\"image\"):\n",
    "            # Load input image and resize\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, (672, 672))  # Replace with your desired size\n",
    "\n",
    "            # Convert image to RGB format and run hand detection\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image)\n",
    "\n",
    "            # Check if hand(s) were detected\n",
    "            if results.multi_hand_landmarks:\n",
    "                # Extract landmarks for detected hand\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Normalize landmarks with respect to image size\n",
    "                    image_height, image_width, _ = image.shape\n",
    "                    landmarks_norm = np.array([[lmk.x * image_width, lmk.y * image_height, lmk.z] for lmk in hand_landmarks.landmark])\n",
    "\n",
    "                    # Draw landmarks on image\n",
    "                    mp_drawing = mp.solutions.drawing_utils\n",
    "                    image_draw = image.copy()\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image_draw, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Draw connections between landmarks with red lines\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image_draw, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                        mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2)\n",
    "                    )\n",
    "\n",
    "                    # Add landmarks and label to lists\n",
    "                    landmarks.append(landmarks_norm.flatten())\n",
    "                    labels.append(subdir)\n",
    "                    \n",
    "                    # Save output image\n",
    "                    output_path = os.path.join(output_dir, subdir, os.path.basename(image_path))\n",
    "                    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                    cv2.imwrite(output_path, image_draw)\n",
    "\n",
    "\n",
    "# Clean up\n",
    "hands.close()\n",
    "print(\"Clean up Done\")\n",
    "\n",
    "# Convert landmarks and labels to NumPy arrays\n",
    "landmarks = np.array(landmarks)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Encode the labels using one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder()\n",
    "labels_one_hot = onehot_encoder.fit_transform(labels_encoded.reshape(-1, 1))\n",
    "\n",
    "if input_dir == \"Letters_Dataset\":\n",
    "    # Save the landmarks and labels files in the \"Letters\" directory\n",
    "    np.save(\"Letters/hand_landmarks.npy\", landmarks)\n",
    "    np.save(\"Letters/hand_labels.npy\", labels)\n",
    "    print(\"Store the extracted hand_landmarks.npy and hand_labels.npy Done\")\n",
    "elif input_dir == \"Numbers_Dataset\":\n",
    "    # Save the landmarks and labels files in the \"Numbers\" directory\n",
    "    np.save(\"Numbers/hand_landmarks.npy\", landmarks)\n",
    "    np.save(\"Numbers/hand_labels.npy\", labels)\n",
    "    print(\"Store the extracted hand_landmarks.npy and hand_labels.npy Done\")\n",
    "else:\n",
    "    np.save(\"hand_landmarks.npy\", landmarks)\n",
    "    np.save(\"hand_labels.npy\", labels)\n",
    "    print(\"Store the extracted hand_landmarks.npy and hand_labels.npy Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a68d0-05b2-4b8b-9021-734600393c3a",
   "metadata": {},
   "source": [
    "Check what's stored in the .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b5a181-efd7-4aec-a090-abfd2eaa2a90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17996, 63)\n",
      "(17996,)\n",
      "[[ 3.37330204e+02  4.56753605e+02  8.95528956e-07]\n",
      " [ 4.09144758e+02  4.23423243e+02 -5.36280125e-02]\n",
      " [ 4.45806078e+02  3.65583921e+02 -6.08321428e-02]\n",
      " [ 4.65226135e+02  3.13405798e+02 -6.60104081e-02]\n",
      " [ 4.77884943e+02  2.59490679e+02 -7.06487969e-02]\n",
      " [ 4.13255533e+02  2.95857940e+02  2.84230113e-02]\n",
      " [ 4.29622530e+02  2.32886719e+02  2.65974123e-02]\n",
      " [ 4.36262655e+02  1.95435908e+02  6.46114070e-03]\n",
      " [ 4.40368464e+02  1.60133882e+02 -1.42610529e-02]\n",
      " [ 3.77124573e+02  2.88625351e+02  3.57130431e-02]\n",
      " [ 3.93832872e+02  2.21093084e+02  3.48498113e-02]\n",
      " [ 4.03848976e+02  1.73748556e+02  2.15861248e-03]\n",
      " [ 4.13155037e+02  1.36242621e+02 -2.89214384e-02]\n",
      " [ 3.40370287e+02  2.90684924e+02  3.09532043e-02]\n",
      " [ 3.57400023e+02  2.28250231e+02  1.84104852e-02]\n",
      " [ 3.71441465e+02  1.91035721e+02 -1.96998902e-02]\n",
      " [ 3.84010031e+02  1.58403585e+02 -5.13120778e-02]\n",
      " [ 3.02293588e+02  2.98379880e+02  1.95959825e-02]\n",
      " [ 3.20032705e+02  2.49842434e+02 -1.14653097e-03]\n",
      " [ 3.32281637e+02  2.20992248e+02 -2.84883548e-02]\n",
      " [ 3.46353041e+02  1.92107134e+02 -5.29083721e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npy file\n",
    "hand_landmarks = np.load('hand_landmarks.npy')\n",
    "hand_labels = np.load('hand_labels.npy')\n",
    "\n",
    "# Print the shape of the data\n",
    "print(hand_landmarks.shape)\n",
    "print(hand_labels.shape)\n",
    "\n",
    "# Reshape data[0] to a 2D array with 3 columns\n",
    "hand_landmarks_table = hand_landmarks[0].reshape(-1, 3)\n",
    "hand_labels_table = hand_labels[0]\n",
    "\n",
    "print(hand_landmarks_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d624d1-ac99-4fdc-9e4f-9b35b2747947",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32aad4a-9daf-4460-9fe4-541f6547ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\r\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "713/713 [==============================] - 2578s 4s/step - loss: 0.5988 - accuracy: 0.8226 - val_loss: 0.1018 - val_accuracy: 0.9663\n",
      "Epoch 2/10\n",
      "713/713 [==============================] - 2746s 4s/step - loss: 0.0812 - accuracy: 0.9765 - val_loss: 0.1168 - val_accuracy: 0.9719\n",
      "Epoch 3/10\n",
      "713/713 [==============================] - 2559s 4s/step - loss: 0.0585 - accuracy: 0.9834 - val_loss: 0.0853 - val_accuracy: 0.9761\n",
      "Epoch 4/10\n",
      "713/713 [==============================] - 2401s 3s/step - loss: 0.0291 - accuracy: 0.9921 - val_loss: 0.0178 - val_accuracy: 0.9953\n",
      "Epoch 5/10\n",
      "713/713 [==============================] - 2382s 3s/step - loss: 0.0520 - accuracy: 0.9852 - val_loss: 0.0659 - val_accuracy: 0.9791\n",
      "Epoch 6/10\n",
      "713/713 [==============================] - 2236s 3s/step - loss: 0.0318 - accuracy: 0.9917 - val_loss: 0.0223 - val_accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "713/713 [==============================] - 2227s 3s/step - loss: 0.0314 - accuracy: 0.9923 - val_loss: 0.0331 - val_accuracy: 0.9925\n",
      "Epoch 8/10\n",
      "713/713 [==============================] - 2225s 3s/step - loss: 0.0204 - accuracy: 0.9947 - val_loss: 0.0113 - val_accuracy: 0.9974\n",
      "Epoch 9/10\n",
      "713/713 [==============================] - 2227s 3s/step - loss: 0.0282 - accuracy: 0.9926 - val_loss: 0.0192 - val_accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "713/713 [==============================] - 2278s 3s/step - loss: 0.0230 - accuracy: 0.9941 - val_loss: 0.0180 - val_accuracy: 0.9960\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "train = \"Letters\"\n",
    "\n",
    "if train == \"Letters\":\n",
    "    # Load the landmark and label data\n",
    "    landmarks = np.load(\"Letters/hand_landmarks.npy\")\n",
    "    labels = np.load(\"Letters/hand_labels.npy\")\n",
    "                        \n",
    "elif train == \"Numbers\":\n",
    "    # Load the landmark and label data\n",
    "    landmarks = np.load(\"Numbers/hand_landmarks.npy\")\n",
    "    labels = np.load(\"Numbers/hand_labels.npy\")                   \n",
    "                        \n",
    "# Normalize the landmark data\n",
    "mean = np.mean(landmarks, axis=0)\n",
    "std = np.std(landmarks, axis=0)\n",
    "landmarks_norm = (landmarks - mean) / std\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "labels_int = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "labels_one_hot = onehot_encoder.fit_transform(labels_int.reshape(-1, 1))\n",
    "\n",
    "# Reshape the landmark data to match the input shape of VGG16\n",
    "landmarks_resized = np.zeros((landmarks_norm.shape[0], 112, 112, 3))\n",
    "for i in range(landmarks_norm.shape[0]):\n",
    "    img = np.stack([landmarks_norm[i]] * 3, axis=-1)\n",
    "    img = np.expand_dims(img, axis=0)  # add a new axis to img\n",
    "    img_resized = tf.image.resize(img, (112, 112)).numpy()[0]  # resize and remove the added axis\n",
    "    landmarks_resized[i] = img_resized\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(landmarks_resized, labels_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and compile the VGG16 model\n",
    "vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(112, 112, 3))\n",
    "x = vgg_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(labels_one_hot.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=vgg_model.input, outputs=predictions)\n",
    "\n",
    "# Reduce the learning rate to make the model converge faster\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model with categorical_crossentropy loss\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "if train == \"Letters\":\n",
    "    #Save the trained model\n",
    "    model.save(\"Letters\\hand_sign_model.h5\")\n",
    "elif train == \"Numbers\":\n",
    "    #Save the trained model\n",
    "    model.save(\"Numbers\\hand_sign_model.h5\")\n",
    "else:\n",
    "    #Save the trained model\n",
    "    model.save(\"hand_sign_model.h5\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779f7a2-f030-4325-9699-c1ce6e7146e1",
   "metadata": {},
   "source": [
    "Convert model to a Tensorflow js model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e107c770-46fd-4e47-8a42-a3a923f2b9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Letter model TFjs conversion Done\n",
      "Number model TFjs conversion Done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "#Convert the model to TensorFlow.js format\n",
    "Letter_model = keras.models.load_model(\"Letters/hand_sign_model.h5\")\n",
    "tfjs.converters.save_keras_model(Letter_model, \"Letters/tfjs_model\")\n",
    "print(\"Letter model TFjs conversion Done\")\n",
    "\n",
    "Number_model = keras.models.load_model(\"Numbers/hand_sign_model.h5\")\n",
    "tfjs.converters.save_keras_model(Number_model, \"Numbers/tfjs_model\")\n",
    "print(\"Number model TFjs conversion Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "726a8552-1c7e-4c3d-82ea-9ac9f4cdfce8",
   "metadata": {},
   "source": [
    "Check the model with a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93727297-5056-4945-ad6f-e51576f80ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "#Load the model and use it for inference\n",
    "loaded_model = tf.keras.models.load_model(\"hand_sign_model.h5\")\n",
    "img = np.random.randn(1, 224, 224, 3)\n",
    "prediction = loaded_model.predict(img)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06af3d2-27f7-418d-8587-2c7a5aae0571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
